<p><img src="/assets/images/template-matching/header-short.png" alt="Before-after image" />
&lt;aside class="sidebar__right"&gt;
&lt;nav class="toc" markdown="1"&gt;</p>
<header><h4 class="nav__title"><i class="fa fa-"></i> Table of Contents</h4></header>
<ul class="toc__menu" id="markdown-toc">
  <li><a href="#todays-awesome-problem">Today’s Awesome Problem</a></li>
  <li><a href="#i-have-no-clue-but-thats-awesome">I Have No Clue, But That’s Awesome</a></li>
  <li><a href="#writing-the-algorithm">Writing the Algorithm</a></li>
  <li><a href="#conclusion">Conclusion</a></li>
  <li><a href="#complete-code">Complete Code</a></li>
  <li><a href="#list-of-references">List of references</a></li>
</ul>
<p>&lt;/nav&gt;
&lt;/aside&gt;
After my first task of developing the Django web application at my summer internship, my second project, to my great excitement, is a lot more algorithmically involved. It involves processing raw data annotations into a format suitable for training Machine Learning algorithms.</p>

<p>Here’s the preamble. My research group is working on training a neural network to detect mitosis (cell division) in breast cancer histological images (See <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3709417/">ICPR 2012 Mitosis Detection Contest</a>). Apparently, mitosis count is a good measurement of the aggressiveness of cancer tumors. But mitosis detection is a very challenging task for a computer, because of its indistinctness and its variability. To the untrained eye, such as mine, a cell undergoing mitosis also looks pretty much the same as a regular cell. There are also four stages of mitosis, each with a different shape configuration. My future work involves improving our mitosis detection model. Exciting, but that is a job for another day. </p>

<h2 id="todays-awesome-problem">Today’s Awesome Problem</h2>
<p>Equally awesome is my current undertaking. Since this is another collaboration between IHPC and one of Singapore’s hospitals, we are training our neural net using data provided by our partner. However, the data was annotated in a very raw format, in the form of these:</p>

<p><img src="/assets/images/template-matching/M21.jpg" alt="Original training image" />
<em>Training data in its original form</em></p>

<p>The pain was two-fold. One, individual cells used for training must be cropped by hand. Two, the neural net would output of a set of pixel coordinates, and having no coordinate information, it was again up to the human eye to compare it to the images with green arrows.</p>

<p>I’m thus armed with the following goal:</p>

<ul>
  <li>Write a program that, given an image with green arrows, outputs the pixel coordinates of where the arrows are pointing. </li>
</ul>

<h2 id="i-have-no-clue-but-thats-awesome">I Have No Clue, But That’s Awesome</h2>
<p>I absolutely love this problem for two reasons: One, it’s meaningful as it drastically reduces human workload. Two, it’s a problem where I have no frickin’ clue how to solve, which in other words, mean it’s the best chance to learn something new!</p>

<p>It is indeed trivial to find the color green in the images. But I need to know where the arrows are pointing, which requires knowing their orientation. I soon realized this is a lot more involved than finding green. Also, I happen to have zero computer vision experience. However, having no clue is what inspires me to tackle a problem, and I quickly set to work figuring out a line of attack. </p>

<p>My first thought was to use a neural net, which I will train to give me the locations of arrow tips. But this felt like a bad solution because it requires a lot of knob-tuning, and it’s a black box. </p>

<p>Computer Vision whizzes on Quora pointed me in the right direction: Template Matching! Initially, I was thrown off by the fact that my arrows had different orientations. Did I have to implement a method that was rotationally-invariant? (<a href="https://link.springer.com/chapter/10.1007/978-3-540-77129-6_13">Kim &amp; Araujo, 2007</a>) In the end, I went with a simple solution that avoided such overkill. I had 360 templates, one for each degree of rotation, and use the template matching methods in openCV. </p>

<p>This would help me place bounding boxes on arrows, as well as the rotation information of the arrows. With a bit of trigonometry, I can then extract information about the actual location the arrows are pointing. </p>

<h2 id="writing-the-algorithm">Writing the Algorithm</h2>
<p>My template matching algorithm is summarized as so:</p>

<ol>
  <li>Strip the background from the images. This improves matching by removing distracting elements. </li>
  <li>Make 360 templates, one for each degree of rotation. </li>
  <li>Match each image with 360 templates. (Fortunately, this is fairly fast for my small templates. Also probably faster rotational-invariant feature matching techniques, which I saw people complain on StackOverflow to be painfully slow)</li>
  <li>Knowing the match locations and orientations, I can transform the points to recover the final location of arrow tips (I will talk about this in Part 2)</li>
</ol>

<p>Step 1 is trivial, as all the images had the same shade of green arrows. All I had to do was zero the non-green vectors in the RGBA matrix. I also did some parallel processing to speed this up for bulk processing of images (see complete code at bottom of page). Step 2 was likewise easy using SciPy, with this as my base template:\
<img src="/assets/images/template-matching/base.png" alt="Base template" /></p>

<pre><code class="language-python">import numpy as np
import os
from scipy import misc, ndimage

IMG_DIR = {path to original images}
STRIPPED_DIR = {path to save stripped images}
TMPL_DIR = {path to templates}

GREEN = np.array([89, 248,  89])

def strip(img_name):
    """ Removes background from img_name in IMG_DIR, leaving only green arrows.
    Saves stripped image in STRIPPED_DIR, as img_name's'.jpg 
    """
    print('Stripping img %s' %img_name)
    arr = misc.imread(os.path.join(IMG_DIR, img_name))
    (x_size, y_size, z_size) = arr.shape
    for x in range(x_size):
        for y in range(y_size):
            if not np.array_equal(arr[x, y], GREEN):
                arr[x, y] = np.array([0, 0, 0])
    img_name = "".join(img_name.split('.')[:-1])
    misc.imsave(os.path.join(STRIPPED_DIR, img_name+'_s.jpg'), arr) #eg M21_s.jpg
    return

def make_templates(base='base.png'):
    """ Makes templates for rotational-deg=0...359 from base in TMPL_DIR.
    Saves rotated templates as tmpl{deg}.png in TMPL_DIR
    """
    base = misc.imread(os.path.join(TMPL_DIR, base))
    for deg in range(360):
        tmpl = ndimage.rotate(base, deg)
        misc.imsave(os.path.join(TMPL_DIR, 'tmpl%d.png' %deg), tmpl)
    return
</code></pre>

<p>Next, following this <a href="http://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_imgproc/py_template_matching/py_template_matching.html">wonderful tutorial</a>, I had step 3 functioning in no time. The algorithm uses the <code>cv2.TM_CCORR_NORMED</code> method on greyscaled images, with a match-acceptance threshold of <code>&gt;0.9</code>. It worked 99% of the time. But, in accordance with the power law of programming, that remaining 1% gave the biggest headache. Some edge cases gave me unacceptable false negatives:</p>

<p><img src="/assets/images/template-matching/falseneg.png" alt="False Negatives" />\
<em>False negatives (cv2.TM_CCORR_NORMED, acceptance threshold &gt;0.9)</em></p>

<p>I could simply lower the threshold, but that gave me a ton of false positives before all the ground truths (ML lingo for the “real, actual positives”) were accepted. Due to the nature of the problem, it had to work 100%, and I needed a perfect solution.</p>

<p>I realized the problem was due to the proximity of the arrows, such that they would overlap with the back region of the templates during matching. The black background in the templates must be somehow excluded from the correlation.</p>

<p>To my dismay, openCV does not support alpha-dependent matching. As reluctant as I was to reinvent the wheel, I mentally prepared myself to write my own matching algorithm. </p>

<p>But then I stumbled upon an article about a new masking feature for openCV 3.2 in C. Aha! I couldn’t find useful information because there simply wasn’t documentation for openCV 3.2 for Python! Masks were my solution.</p>

<p>To create masks for each template, I used openCV’s <a href="http://docs.opencv.org/trunk/d7/d4d/tutorial_py_thresholding.html">thresholding feature</a>, which converted my templates from grayscale to black and white. The black regions acted as the “opaque” mask, allowing the white “transparent” regions to be factored into the correlation calculations.</p>

<p><img src="/assets/images/template-matching/mask.png" alt="Mask" />\
<em>Example of a template after thresholding</em></p>

<p>Tuning the masks to work with openCV’s <code>matchTemplate()</code> method was another issue for me. Although <code>TM_CCORR_NORMED</code> now correctly identified my true positives, it gave me false positives that are even stronger than the true ones. </p>

<p><img src="/assets/images/template-matching/falsepos.png" alt="False positives" />\
<em>False positives (cv2.TM_CCORR_NORMED, acceptance threshold &gt;0.9)</em></p>

<p>To this end, the wonderful folks on <a href="https://stackoverflow.com/questions/44690002/python-opencv-matchtemplate-on-grayscale-image-with-masking/44693722?noredirect=1#comment76390884_44693722]">StackOverflow helped me tremendously</a>. Using, the matching method <code>TM_SQDIFF</code> and a new threshold of <code>&lt;11</code> (empirically tuned), and using png templates and masks instead of jpeg did the trick. Note that non-matches have correlation of <code>&gt;200</code>, so a acceptance threshold of 11 is really good.</p>

<p><img src="/assets/images/template-matching/M21_r.jpg" alt="Success" />\
<em>Success! (cv2.TM_SQDIFF, acceptance threshold &lt;11)</em></p>

<p>Cool was learning why <code>TM_CCORR_NORMED</code> did not work with jpeg.</p>

<script type="math/tex; mode=display">R_{\textrm{ccorr normed}}(x, y) = \frac{\sum_{x',y'}(T(x', y') \cdotp I(x+x', y+y'))}{\sqrt{\sum_{x',y'}T(x', y')^2 \cdotp I(x+x', y+y')^2}}</script>

<script type="math/tex; mode=display">R_{\textrm{sqdiff}}(x, y) = \sum_{x', y'}(T'(x', y') \cdotp I(x+x', y+y')) </script>

<p>Because <code>TM_CCORR_NORMED</code> is a normalized function with the denominator that it has, the black backgrounds meant I was dividing by 0 in many cases. This is not cool, but having a matrix entry of <code>numpy.nan</code> wasn’t the issue because the comparison <code>nan&gt;threshold</code> yields false. The key was with the jpeg format. Because of lossy jpeg compression, entries that should have been 0 (black), were not. This gave me very small denominators, causing the correlation to blow up and give me false positives. This is why png masks, or using <code>TM_SQDIFF</code> which does not have the denominator, fixes the problem.</p>

<p>Here is the end result and code, using <code>TM_SQDIFF</code> and png masks:</p>

<pre><code class="language-python">import cv2

MASK_DIR = {path to masks}
RES_DIR = {path to save match results}

MATCH_THRESH = 11
MATCH_RES = 1  #specifies degree-interval at which to match
#Match thresholds and resolution were empirically tuned

def make_masks():
    """ Makes masks from tmpl{0...359}.png in TMPL_DIR.
    Saves masks as mask{0...359}.png in MASK_DIR
    """
    for deg in range(360):
        tmpl = cv2.imread(os.path.join(TMPL_DIR, 'tmpl%d.png' %deg), 
            cv2.IMREAD_GRAYSCALE)
        ret2, mask = cv2.threshold(tmpl, 0, 255, 
            cv2.THRESH_BINARY+cv2.THRESH_OTSU)
        cv2.imwrite(os.path.join(MASK_DIR, 'mask%d.png' %deg), mask)
    return

def match_and_draw(img_name):
    """ Apply template matching to img_name in IMG_DIR, using
    tmpl{0...359}.png and mask{0...359}.png.
    Saves result with boxes drawn around matches as as img_name'r'.jpg in RES_DIR
    """
    print('Matching img %s' %img_name)
    img_rgb = cv2.imread(os.path.join(STRIPPED_DIR, img_name))	img_gray = cv2.cvtColor(img_rgb, cv2.COLOR_BGR2GRAY)
    for deg in range(0, 360, MATCH_RES):
        tmpl = cv2.imread(os.path.join(TMPL_DIR, 'tmpl%d.png' %deg), 
            cv2.IMREAD_GRAYSCALE)
        mask = cv2.imread(os.path.join(MASK_DIR, 'mask%d.png' %deg), 
            cv2.IMREAD_GRAYSCALE)

        w, h = tmpl.shape[::-1]
        res = cv2.matchTemplate(img_gray, tmpl, cv2.TM_SQDIFF, mask=mask)
        loc = np.where(res &lt; MATCH_THRESH)
        for pt in zip(*loc[::-1]):
            cv2.rectangle(img_rgb, pt, (pt[0] + w, pt[1] + h), (0,0,255), 2)
            print('Match for deg{}, pt({}, {}), sqdiff {}'.
                format(deg,pt[0],pt[1],res[pt[1],pt[0]]))
    img_name = "".join(img_name.split('_')[:-1])
    cv2.imwrite(os.path.join(RES_DIR, img_name+'_r.jpg'), img_rgb) #eg M21_r.jpg
    return
</code></pre>

<p>Again, a method for bulk processing of images is in the complete code at the bottom of this page.</p>

<h2 id="conclusion">Conclusion</h2>
<p>Now, I have completed my first goal of locating all the arrows. Of course, the current code only finds and draws bounding boxes on images, which is useful for debugging purposes. It is easy to modify my code to extract the coordinates of the boxes, which tells me roughly where my arrows are (the top left corner of the boxes are given by the <code>pt</code> variable in the code block above). </p>

<p>My next step is to transform these points, using the orientation information of the matching templates, into pixels that the arrows point to, which will be our ground truth. This doesn’t have to be exact of course, since we only need to tell our neural net roughly where to look, and if the ground-truth pixel is within the patch returned by the neural net. </p>

<p>Also, notice that the current program may output a couple of matching points for each arrow. I would need to coalesce these points into a single point for an accurate mitosis count. I will talk about my point transformation and coalescing algorithm in <strong>Part 2</strong>!</p>

<h2 id="complete-code">Complete Code</h2>
<p>Below is the complete code for Part 1, with added error handling and print statements.</p>

<pre><code class="language-python">import os
import re
import cv2
import numpy as np
from scipy import misc, ndimage
from multiprocessing import Pool

IMG_DIR = {path to original images}
STRIPPED_DIR = {path to save stripped images}
TMPL_DIR = {path to templates}
MASK_DIR = {path to masks}
RES_DIR = {path to save match results}

GREEN = np.array([89, 248,  89])
MATCH_THRESH = 11
MATCH_RES = 1  #specifies degree-interval at which to match
#Match thresholds and resolution were empirically tuned

def strip(img_name):
    """ Removes background from img_name in IMG_DIR, leaving only green arrows.
    Saves stripped image in STRIPPED_DIR, as img_name's'.jpg """
    print('Stripping img %s' %img_name)
    try:
        arr = misc.imread(os.path.join(IMG_DIR, img_name))
    except IOError:
        print('Failed to strip img %s. Image not found.' %img_name)
        return
    (x_size, y_size, z_size) = arr.shape
    for x in range(x_size):
        for y in range(y_size):
            if not np.array_equal(arr[x, y], GREEN):
                arr[x, y] = np.array([0, 0,  0])
    img_name = "".join(img_name.split('.')[:-1])
    misc.imsave(os.path.join(STRIPPED_DIR, img_name+'_s.jpg'), arr) #eg M21_s.jpg
    return

def strip_all(num_processes=2):
    """ Applies strip() to all images in IMG_DIR.

    This method uses multiprocessing:
    num_processes -- the number of parallel processes to spawn for this task.
    (default 2)
    """
    imgs = [i for i in os.listdir(IMG_DIR) if re.match(r'M[0-9]*.jpg', i)]
    print('Stripping background from %d images' %len(imgs))
    pool = Pool(num_processes)
    pool.map(strip, imgs)
    pool.close()
    pool.join()
    print('Done')
    return

def make_templates():
    """ Makes templates for rotational-deg=0...359 from base.jpg in TMPL_DIR.
    Saves rotated templates as tmpl{deg}.png in TMPL_DIR
    """
    try:
        base = misc.imread(os.path.join(TMPL_DIR,'base_short.png'))
    except IOError:
        print('Failed to make templates. Base template is not found')
        return
    for deg in range(360):
        tmpl = ndimage.rotate(base, deg)
        misc.imsave(os.path.join(TMPL_DIR, 'tmpl%d.png' %deg), tmpl)
    return

def make_masks():
    """ Makes masks from tmpl{0...359}.png in TMPL_DIR.
    Saves masks as mask{0...359}.png in MASK_DIR
    """
    for deg in range(360):
        tmpl = cv2.imread(os.path.join(TMPL_DIR, 'tmpl%d.png' %deg), 
            cv2.IMREAD_GRAYSCALE)
        if tmpl is None:
            print('Failed to make mask {0}. tmpl{0}.png is not found.'.
                format(deg))
        else:
            ret2, mask = cv2.threshold(tmpl, 0, 255, 
                cv2.THRESH_BINARY+cv2.THRESH_OTSU)
            cv2.imwrite(os.path.join(MASK_DIR, 'mask%d.png' %deg), mask)
    return

def match_and_draw(img_name):
    """ Applies template matching to img_name in IMG_DIR, using
    tmpl{0...359}.png and mask{0...359}.png.
    Saves result with boxes drawn around matches as as img_name'r'.jpg in RES_DIR
    """
    print('Matching img %s' %img_name)
    img_rgb = cv2.imread(os.path.join(STRIPPED_DIR, img_name))
    if img_rgb is None:
        print('Failed to match img %s. Image not found.' %img_name)
        return
    img_gray = cv2.cvtColor(img_rgb, cv2.COLOR_BGR2GRAY)
    for deg in range(0, 360, MATCH_RES):
        tmpl = cv2.imread(os.path.join(TMPL_DIR, 'tmpl%d.png' %deg), 
            cv2.IMREAD_GRAYSCALE)
        mask = cv2.imread(os.path.join(MASK_DIR, 'mask%d.png' %deg), 
            cv2.IMREAD_GRAYSCALE)
        if tmpl is None or mask is None:
            print('Failed to match for tmpl %d.' %deg)
        else:
            w, h = tmpl.shape[::-1]
            res = cv2.matchTemplate(img_gray, tmpl, cv2.TM_SQDIFF, mask=mask)
            loc = np.where(res &lt; MATCH_THRESH)
            for pt in zip(*loc[::-1]):
                cv2.rectangle(img_rgb, pt, (pt[0] + w, pt[1] + h), (0,0,255), 2)
                print('Match for deg{}, pt({}, {}), sqdiff {}'.
                    format(deg,pt[0],pt[1],res[pt[1],pt[0]]))
    img_name = "".join(img_name.split('_')[:-1])
    cv2.imwrite(os.path.join(RES_DIR, img_name+'_r.jpg'), img_rgb) #eg M21_r.jpg
    return

def match_all(num_imgs, start=1, num_processes=2):
    """ Applies match() to all images in STRIPPED_DIR.

    This method uses multiprocessing:
    num_processes -- the number of parallel processes to spawn for this task.
    (default 2)
    """
    imgs = [i for i in os.listdir(STRIPPED_DIR) if re.match(r'M[0-9]*_s.jpg', i)]
    print('Matching %d images' %len(imgs))
    pool = Pool(num_processes)
    pool.map(match_and_draw, imgs)
    pool.close()
    pool.join()
    print('Done')
    return
</code></pre>

<h2 id="list-of-references">List of references</h2>
<ol>
  <li><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3709417/">ICPR 2012 Mitosis Detection Contest</a></li>
  <li><a href="https://link.springer.com/chapter/10.1007/978-3-540-77129-6_13">2007 Paper on Grayscale Template-Matching Invariant to Rotation, Scale, Translation, Brightness and Contrast (Kim &amp; Araujo)</a></li>
  <li><a href="http://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_imgproc/py_template_matching/py_template_matching.html">OpenCV template-matching tutorial in Python</a></li>
  <li><a href="http://docs.opencv.org/trunk/d7/d4d/tutorial_py_thresholding.html">OpenCV thresholding tutorial in Python</a></li>
  <li><a href="https://stackoverflow.com/questions/44690002/python-opencv-matchtemplate-on-grayscale-image-with-masking/44693722?noredirect=1#comment76390884_44693722]">Helpful StackOverflow explanation of OpenCV’s different matching methods</a></li>
</ol>

